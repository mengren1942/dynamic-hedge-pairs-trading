# pairs/models/kalman.py
"""
Kalman-based dynamic hedge ratio fitting (joblib-parallel) plus residual
stationarity summaries. Designed for a MultiIndex (ticker, datetime) DataFrame
with a 'close' column. No side effects at import.

Public API:
- kalman_dynamic_hedge_joblib(...)
- summarize_spread_stationarity_joblib(...)
- fit_kalman_hedge(...)  # alias to kalman_dynamic_hedge_joblib for top-level convenience
"""

from __future__ import annotations

# --- typing & stdlib ---
from itertools import combinations
from typing import Optional, Literal, Dict, Tuple, List
from contextlib import contextmanager
import os
import warnings

# --- third-party ---
import numpy as np
import pandas as pd
from joblib import Parallel, delayed
import joblib
from tqdm import tqdm

# pykalman
from pykalman import KalmanFilter

# statsmodels (only for stationarity summary helpers)
from statsmodels.tools.sm_exceptions import InterpolationWarning
from statsmodels.tsa.stattools import adfuller, kpss

__all__ = [
    "tqdm_joblib",
    "kalman_dynamic_hedge_joblib",
    "summarize_spread_stationarity_joblib",
    "estimate_halflife",
    "test_spread_stationarity",
    "fit_kalman_hedge",  # alias
]

# ---------- progress helper for joblib + tqdm ----------
@contextmanager
def tqdm_joblib(tqdm_object):
    class TqdmBatchCallback(joblib.parallel.BatchCompletionCallBack):
        def __call__(self, *args, **kwargs):
            tqdm_object.update(n=self.batch_size)
            return super().__call__(*args, **kwargs)
    old_cb = joblib.parallel.BatchCompletionCallBack
    joblib.parallel.BatchCompletionCallBack = TqdmBatchCallback
    try:
        yield tqdm_object
    finally:
        joblib.parallel.BatchCompletionCallBack = old_cb
        tqdm_object.close()

# ---------- helpers ----------
def _align_pair_multiindex(data: pd.DataFrame, k1: str, k2: str) -> pd.DataFrame:
    """
    Align two close-price series from a MultiIndex (ticker, datetime) DataFrame.
    Returns a float64 DataFrame with columns ['P1','P2'] on the datetime index.
    """
    # Expect: first level 'ticker', second level 'datetime'
    if not isinstance(data.index, pd.MultiIndex) or data.index.nlevels < 2:
        raise ValueError("data must have a MultiIndex with (ticker, datetime).")
    if "close" not in data.columns:
        raise ValueError("data must include a 'close' column.")

    # Slice by ticker level, drop ticker level to leave only datetime, then align
    s1 = data.loc[(k1,), "close"]
    s2 = data.loc[(k2,), "close"]
    if isinstance(s1.index, pd.MultiIndex):
        s1 = s1.droplevel(0)
    if isinstance(s2.index, pd.MultiIndex):
        s2 = s2.droplevel(0)

    # Ensure datetime index and sort
    if not isinstance(s1.index, pd.DatetimeIndex):
        s1.index = pd.to_datetime(s1.index, errors="coerce")
    if not isinstance(s2.index, pd.DatetimeIndex):
        s2.index = pd.to_datetime(s2.index, errors="coerce")
    s1 = s1.sort_index()
    s2 = s2.sort_index()

    df = pd.concat([s1.rename("P1"), s2.rename("P2")], axis=1).dropna()
    return df.astype(np.float64)

def _kalman_dynamic_hedge(
    k1: str,
    k2: str,
    df: pd.DataFrame,
    *,
    q: float = 1e-5,
    r: float = 1.0,
    init_cov: float = 1e6,
    mode: Literal["smooth", "filter"] = "smooth",
    em_iters: int = 0,
    return_params: bool = False,
):
    if df is None or len(df) < 5:
        return k1, k2, None, None

    y = df["P1"].values.reshape(-1, 1)
    x = df["P2"].values
    n = len(df)

    # H_t = [[x_t, 1]]
    H = np.zeros((n, 1, 2), dtype=np.float64)
    H[:, 0, 0] = x
    H[:, 0, 1] = 1.0
    F = np.eye(2, dtype=np.float64)

    kf = KalmanFilter(
        transition_matrices=F,
        observation_matrices=H,
        initial_state_mean=np.array([0.0, 0.0], dtype=np.float64),   # [beta0, alpha0]
        initial_state_covariance=np.eye(2, dtype=np.float64) * init_cov,
        transition_covariance=np.eye(2, dtype=np.float64) * q,
        observation_covariance=np.array([[r]], dtype=np.float64),
    )

    if em_iters and em_iters > 0:
        kf = kf.em(y, n_iter=em_iters)  # learns Q,R (and possibly init covs)

    if mode == "smooth":
        state_means, state_covs = kf.smooth(y)
        if return_params:
            f_means, f_covs = kf.filter(y)
            last_mean, last_cov = f_means[-1].copy(), f_covs[-1].copy()
    else:
        state_means, state_covs = kf.filter(y)
        last_mean, last_cov = state_means[-1].copy(), state_covs[-1].copy()

    beta_t  = state_means[:, 0]
    alpha_t = state_means[:, 1]
    y_hat   = alpha_t + beta_t * x
    resid   = df["P1"].values - y_hat

    states_df = pd.DataFrame(
        {"alpha": alpha_t, "beta": beta_t, "y_hat": y_hat, "resid": resid},
        index=df.index,  # datetime index
    )

    params = None
    if return_params:
        params = {
            "F": F.copy(),
            "Q": kf.transition_covariance.copy(),
            "R": kf.observation_covariance.copy(),
            "last_state_mean": last_mean,
            "last_state_cov": last_cov,
            "mode": mode,
            "em_iters": int(em_iters),
        }

    return k1, k2, states_df, params

# ---------- main API ----------
def kalman_dynamic_hedge_joblib(
    data: pd.DataFrame,
    pairs: Optional[List[Tuple[str, str]]] = None,
    *,
    q: float = 1e-5,
    r: float = 1.0,
    init_cov: float = 1e6,
    mode: Literal["smooth", "filter"] = "smooth",
    em_iters: int = 0,
    require_full_span: bool = False,
    n_workers: Optional[int] = None,
    chunksize: int = 100,
    show_progress: bool = True,
    return_params: bool = False,
):
    """
    Expects `data` with MultiIndex (ticker, datetime) and a 'close' column.

    Returns:
      - if return_params=False: dict[(k1,k2)] -> states DataFrame (index=datetime)
      - if return_params=True : (states_dict, params_dict)
    """
    # Avoid BLAS oversubscription
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS",      "1")
    os.environ.setdefault("NUMEXPR_NUM_THREADS",  "1")

    if "close" not in data.columns:
        raise ValueError("data must include a 'close' column.")
    if not isinstance(data.index, pd.MultiIndex) or data.index.nlevels < 2:
        raise ValueError("data must have a MultiIndex with levels (ticker, datetime).")

    # Normalize level names and sort
    names = list(data.index.names)
    if names[0] != "ticker" or names[1] != "datetime":
        new_names = names[:]
        new_names[0] = "ticker"
        new_names[1] = "datetime"
        data = data.copy()
        data.index = data.index.set_names(new_names)
    data = data.sort_index(level=["ticker", "datetime"])

    # Native Python strings for tickers
    tickers: List[str] = [str(k) for k in data.index.get_level_values("ticker").unique().tolist()]

    # Build the list of pairs to evaluate
    if pairs is None:
        idx_pairs: List[Tuple[str, str]] = list(combinations(tickers, 2))
    else:
        # ensure native str
        idx_pairs = [(str(a), str(b)) for (a, b) in pairs]

    # Optionally require full-span coverage on the datetime level
    if require_full_span:
        dt = data.index.get_level_values("datetime")
        full_start, full_end = dt.min(), dt.max()

        def _full_span_ok(k: str) -> bool:
            s = data.loc[(k,), "close"]
            if isinstance(s.index, pd.MultiIndex):
                s = s.droplevel(0)
            return (len(s) > 0) and (s.index.min() == full_start) and (s.index.max() == full_end)

        idx_pairs = [(a, b) for (a, b) in idx_pairs if _full_span_ok(a) and _full_span_ok(b)]

    n_workers = n_workers or os.cpu_count() or 1

    def worker(k1: str, k2: str):
        df_ab = _align_pair_multiindex(data, k1, k2)
        return _kalman_dynamic_hedge(
            k1, k2, df_ab,
            q=q, r=r, init_cov=init_cov,
            mode=mode, em_iters=em_iters,
            return_params=return_params,
        )

    iterator = (delayed(worker)(k1, k2) for (k1, k2) in idx_pairs)

    if show_progress:
        with tqdm_joblib(tqdm(total=len(idx_pairs), desc="Kalman (α_t, β_t)", leave=False)):
            results = Parallel(n_jobs=n_workers, prefer="processes", batch_size=chunksize)(iterator)
    else:
        results = Parallel(n_jobs=n_workers, prefer="processes", batch_size=chunksize)(iterator)

    states_out: Dict[Tuple[str, str], pd.DataFrame] = {}
    params_out: Dict[Tuple[str, str], dict] = {}

    for k1, k2, df_res, params in results:
        if df_res is not None:
            states_out[(k1, k2)] = df_res
            if return_params and params is not None:
                params_out[(k1, k2)] = params

    if return_params:
        return states_out, params_out
    return states_out

# ----------------- Half-life estimation -----------------
def estimate_halflife(resid: pd.Series) -> float:
    """Estimate half-life of mean reversion from residual series."""
    r = pd.Series(resid).dropna().astype(float)
    if len(r) < 20:
        return np.nan
    r_lag = r.shift(1).dropna()
    y = r.loc[r_lag.index]
    x = r_lag
    # OLS y = a + b x
    X = np.column_stack([np.ones(len(x)), x.values])
    beta = np.linalg.lstsq(X, y.values, rcond=None)[0]
    b = float(beta[1])
    if b <= 0 or b >= 1 or not np.isfinite(b):
        return np.nan
    return -np.log(2.0) / np.log(b)

# ----------------- Stationarity tests -------------------
def test_spread_stationarity(spread: pd.Series, alpha: float = 0.05, regression: str = "c") -> dict:
    s = pd.Series(spread).astype(float).dropna()
    adf_stat, adf_p, adf_lags, adf_nobs, adf_crit, *_ = adfuller(s, autolag="AIC", regression=regression)
    kpss_stat, kpss_p, kpss_lags, kpss_crit = kpss(
        s, regression="c" if regression == "c" else "ct", nlags="auto"
    )
    adf_reject  = adf_p < alpha
    kpss_reject = kpss_p < alpha
    if adf_reject and not kpss_reject:
        verdict = "stationary"
    elif (not adf_reject) and kpss_reject:
        verdict = "non-stationary"
    else:
        verdict = "inconclusive"
    return {
        "adf_stat": adf_stat, "adf_p": adf_p,
        "adf_lags": adf_lags, "adf_nobs": adf_nobs, "adf_crit": adf_crit,
        "kpss_stat": kpss_stat, "kpss_p": kpss_p,
        "kpss_lags": kpss_lags, "kpss_crit": kpss_crit,
        "verdict": verdict
    }

def _pair_stationarity_worker(
    k1: str, k2: str, resid_values: np.ndarray,
    alpha: float, regression: str, suppress_warnings: bool = True
) -> Optional[dict]:
    s = pd.Series(resid_values)
    if s.dropna().empty:
        return None
    if suppress_warnings:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", InterpolationWarning)
            warnings.simplefilter("ignore", UserWarning)
            warnings.simplefilter("ignore", RuntimeWarning)
            res = test_spread_stationarity(s, alpha=alpha, regression=regression)
    else:
        res = test_spread_stationarity(s, alpha=alpha, regression=regression)
    hl = estimate_halflife(s)
    return {
        "ticker1": k1, "ticker2": k2,
        "adf_stat": res["adf_stat"], "adf_p": res["adf_p"],
        "kpss_stat": res["kpss_stat"], "kpss_p": res["kpss_p"],
        "verdict": res["verdict"],
        "halflife": hl,
    }

# ----------------- Main API: stationarity summary -----------------------------
def summarize_spread_stationarity_joblib(
    kf_results: Dict[Tuple[str, str], pd.DataFrame],
    *,
    alpha: float = 0.05,
    regression: str = "c",
    n_workers: Optional[int] = None,
    chunksize: int = 256,
    show_progress: bool = True,
    suppress_warnings: bool = True,
) -> pd.DataFrame:
    """
    Parallel summary of ADF+KPSS stationarity tests for 'resid'
    plus half-life estimation and residual sigma.
    Verdict is last column.
    """
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS",      "1")
    os.environ.setdefault("NUMEXPR_NUM_THREADS",  "1")

    n_workers = n_workers or os.cpu_count()

    tasks = []
    for (k1, k2), df in kf_results.items():
        if "resid" not in df.columns:
            continue
        resid_values = df["resid"].to_numpy(dtype=float, copy=False)
        tasks.append((k1, k2, resid_values))

    def worker_with_sigma(k1, k2, resid_values):
        s = pd.Series(resid_values)
        if s.dropna().empty:
            return None
        if suppress_warnings:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", InterpolationWarning)
                warnings.simplefilter("ignore", UserWarning)
                warnings.simplefilter("ignore", RuntimeWarning)
                res = test_spread_stationarity(s, alpha=alpha, regression=regression)
        else:
            res = test_spread_stationarity(s, alpha=alpha, regression=regression)

        # Half-life estimation
        halflife = np.nan
        try:
            spread_lag = s.shift(1).dropna()
            spread_ret = s.diff().dropna()
            beta = np.polyfit(spread_lag, spread_ret, 1)[0]
            halflife = -np.log(2) / beta if beta != 0 else np.nan
        except Exception:
            pass

        resid_sigma = float(s.std())

        return {
            "ticker1": k1, "ticker2": k2,
            "adf_stat": res["adf_stat"], "adf_p": res["adf_p"],
            "kpss_stat": res["kpss_stat"], "kpss_p": res["kpss_p"],
            "halflife": halflife,
            "resid_sigma": resid_sigma,
            "verdict": res["verdict"],
        }

    iterator = (
        delayed(worker_with_sigma)(k1, k2, resid_values)
        for (k1, k2, resid_values) in tasks
    )

    if show_progress:
        with tqdm_joblib(tqdm(total=len(tasks), desc="Testing stationarity", leave=False)):
            results = Parallel(n_jobs=n_workers, prefer="processes", batch_size=chunksize)(iterator)
    else:
        results = Parallel(n_jobs=n_workers, prefer="processes", batch_size=chunksize)(iterator)

    rows = [r for r in results if r is not None]
    if not rows:
        return pd.DataFrame(
            columns=["adf_stat","adf_p","kpss_stat","kpss_p","halflife","resid_sigma","verdict"],
            index=pd.MultiIndex.from_tuples([], names=["ticker1","ticker2"])
        )

    out = pd.DataFrame(rows).set_index(["ticker1", "ticker2"])

    # Ensure column order: halflife and resid_sigma before verdict
    col_order = ["adf_stat", "adf_p", "kpss_stat", "kpss_p", "halflife", "resid_sigma", "verdict"]
    out = out[col_order]

    # Sort by verdict then adf_p
    out = out.sort_values(["verdict", "adf_p"])
    return out

# --------- Alias for top-level convenience (matches pairs/__init__.py mapping) ---------
def fit_kalman_hedge(*args, **kwargs):
    """Alias to kalman_dynamic_hedge_joblib for backward/forward compatibility."""
    return kalman_dynamic_hedge_joblib(*args, **kwargs)
